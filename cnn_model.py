# -*- coding: utf-8 -*-
"""CNN_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ceqQ_ZSxoP48shh-0u-5tRY9xzFbNmjV
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip install basemap
!pip install mat73
!pip install efficientnet
!pip install keras
!pip install netCDF4

!pip install tensorflow==2.15.0

import matplotlib.pyplot as plt
import numpy as np
import netCDF4 as nc
import tarfile
import os
import numpy.ma as ma
import pandas as pd
from scipy.interpolate import griddata
#from keras.utils import np_utils

from netCDF4 import Dataset
from mpl_toolkits.basemap import Basemap
from mpl_toolkits.axes_grid1 import ImageGrid
import datetime
from pathlib import Path
import scipy.io as io
import mat73

from keras.models import Sequential
from keras import backend as K
from keras import initializers, constraints, regularizers, layers
#from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model, load_model
from keras.layers import Input, Dropout, Conv2D, BatchNormalization, add, LeakyReLU, UpSampling2D
from keras.layers import Conv2DTranspose, Dense, Flatten, Concatenate,concatenate, MaxPooling2D
from keras.losses import binary_crossentropy

import efficientnet.keras as efn
from keras import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from keras.optimizers import SGD
import tensorflow as tf
from tensorflow import keras
from numpy import pad
from collections import OrderedDict

"""#Models

##Unet
"""

input_shape = (256, 256, 2)
input_data = Input(shape=input_shape, name='input_data')

# U-Net Encoder
conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_data)
conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)
conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)
pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

# Bottleneck layer
conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

# U-Net Decoder
up4 = UpSampling2D(size=(2, 2))(conv5)
up4 = concatenate([conv4, up4], axis=-1)
conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(up4)
conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)

up3 = UpSampling2D(size=(2, 2))(conv6)
up3 = concatenate([conv3, up3], axis=-1)
conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(up3)
conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)

up2 = UpSampling2D(size=(2, 2))(conv7)
up2 = concatenate([conv2, up2], axis=-1)
conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(up2)
conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)

up1 = UpSampling2D(size=(2, 2))(conv8)
up1 = concatenate([conv1, up1], axis=-1)
conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1)
conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)

# Output layer for precipitation rate estimation
output_data = Conv2D(1, (1, 1), activation='relu')(conv9)

# Define the model
model = Model(inputs=input_data, outputs=output_data)
model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))

# Print model summary
model.summary()

OUTPUT_CHANNELS = 1
def downsample(filters, size, apply_batchnorm=True):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                             kernel_initializer=initializer, use_bias=False))

  if apply_batchnorm:
    result.add(tf.keras.layers.BatchNormalization())

  result.add(tf.keras.layers.LeakyReLU())

  return result

# Define the upsampler (decoder):
def upsample(filters, size, apply_dropout=False):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                    padding='same',
                                    kernel_initializer=initializer,
                                    use_bias=False))

  result.add(tf.keras.layers.BatchNormalization())

  if apply_dropout:
      result.add(tf.keras.layers.Dropout(0.5))

  result.add(tf.keras.layers.ReLU())

  return result

#  Define the generator with the downsampler and the upsampler:
def Generator():
  inputs = tf.keras.layers.Input(shape=[256, 256, 2])

  down_stack = [
    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)
    downsample(128, 4),  # (batch_size, 64, 64, 128)
    downsample(256, 4),  # (batch_size, 32, 32, 256)
    downsample(512, 4),  # (batch_size, 16, 16, 512)
    downsample(512, 4),  # (batch_size, 8, 8, 512)
    downsample(512, 4),  # (batch_size, 4, 4, 512)
    # downsample(512, 4),  # (batch_size, 2, 2, 512)
    # downsample(512, 4),  # (batch_size, 1, 1, 512)
  ]
  up_stack = [
    # upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)
    # upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)
    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)
    upsample(512, 4),  # (batch_size, 16, 16, 1024)
    upsample(256, 4),  # (batch_size, 32, 32, 512)
    upsample(128, 4),  # (batch_size, 64, 64, 256)
    upsample(64, 4),  # (batch_size, 128, 128, 128)
  ]

  initializer = tf.random_normal_initializer(0., 0.02)
  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=initializer,
                                         activation='relu')
  x = inputs

  # Downsampling through the model
  skips = []
  for down in down_stack:
    x = down(x)
    skips.append(x)

  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    x = tf.keras.layers.Concatenate()([x, skip])

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)

model = Generator()
model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))

# Print model summary
model.summary()

"""##EfficientUNet"""

###EfficientUNetB4
!pip install tensorflow-addons
import tensorflow_addons as tfa
def H(lst, name, use_gn=False):
    if use_gn:
        norm = GroupNormalization(groups=1, name=name+'_gn')
    else:
        norm = BatchNormalization(name=name+'_bn')

    x = concatenate(lst)
    num_filters = int(x.shape.as_list()[-1]/2)

    x = Conv2D(num_filters, (3, 3), padding='same', name=name)(x)
    x = norm(x)
    x = LeakyReLU(alpha=0.1, name=name+'_activation')(x)

    return x

def U(x, use_gn=False):
    if use_gn:
        norm = GroupNormalization(groups=1)
    else:
        norm = BatchNormalization()

    num_filters = int(x.shape.as_list()[-1]/2)

    x = Conv2DTranspose(num_filters, (3, 3), strides=(2, 2), padding='same')(x)
    x = norm(x)
    x = LeakyReLU(alpha=0.1)(x)

    return x

def EfficientUNet(input_shape):
    backbone = efn.EfficientNetB5(
        weights=None,
        include_top=True,
        pooling = 'max',
        input_shape=input_shape
    )

    input = backbone.input
    x00 = backbone.input  # (384,384,2)
    x10 = backbone.get_layer('stem_activation').output  # (128, 256, 4)
    x20 = backbone.get_layer('block2d_add').output  # (64, 128, 32)
    x30 = backbone.get_layer('block3d_add').output  # (32, 64, 56)
    x40 = backbone.get_layer('block5f_add').output  # (16, 32, 160)
    x50 = backbone.get_layer('block7b_add').output  # (8, 16, 448)

    x01 = H([x00, U(x10)], 'X01')
    x11 = H([x10, U(x20)], 'X11')
    x21 = H([x20, U(x30)], 'X21')
    x31 = H([x30, U(x40)], 'X31')
    x41 = H([x40, U(x50)], 'X41')

    x02 = H([x00, x01, U(x11)], 'X02')
    x12 = H([x11, U(x21)], 'X12')
    x22 = H([x21, U(x31)], 'X22')
    x32 = H([x31, U(x41)], 'X32')

    x03 = H([x00, x01, x02, U(x12)], 'X03')
    x13 = H([x12, U(x22)], 'X13')
    x23 = H([x22, U(x32)], 'X23')

    x04 = H([x00, x01, x02, x03, U(x13)], 'X04')
    x14 = H([x13, U(x23)], 'X14')

    x05 = H([x00, x01, x02, x03, x04, U(x14)], 'X05')

    x_out = Concatenate(name='bridge')([x01, x02, x03, x04, x05])
    x_out = Conv2D(1, (3,3), padding="same", name='final_output', activation=tfa.activations.mish)(x_out)
    #x_out = Conv2D(1, (3,3), padding="same", name='final_output', activation='relu')(x_out)

    return Model(inputs=input, outputs=x_out)

model = EfficientUNet((256, 256,1))

model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))

print(model.summary())

"""#Unet Variants"""

!pip install keras-unet-collection
from keras_unet_collection import models

model = models.att_unet_2d((256, 256, 2), [64, 128, 256, 512], n_labels=1,
                           stack_num_down=2, stack_num_up=2,
                           activation='ReLU', atten_activation='ReLU', attention='add', output_activation='ReLU',
                           batch_norm=True, pool=False, unpool='bilinear', name='attunet')

model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='att_unet.png', show_shapes=True)

"""#Data

##3-hourly
"""

#Select rain images
X=np.load(r'/content/gdrive/MyDrive/Colab Notebooks/CNN_Efficient_3_17_2023/Model input/X_3hour_256.npy')
Y=np.load(r'/content/gdrive/MyDrive/Colab Notebooks/CNN_Efficient_3_17_2023/Model input/Y_3hour_256.npy')
print(X.shape,Y.shape)

Xtrain = np.zeros((X.shape[0],256,256,2))
Ytrain = np.zeros((Y.shape[0],256,256,1))
for i in range(X.shape[0]):
   Xtrain[i]=X[i,:256,:256,:]
   Ytrain[i,...,0]=Y[i,:256,:256]
X,Y=[],[]


x_train1,x_val1,y_train,y_val=train_test_split(Xtrain,Ytrain,test_size=0.25)
x_train1.shape

"""##Daily"""

#daily winter
X=np.load(r'/content/gdrive/MyDrive/Colab Notebooks/CNN_Efficient_3_17_2023/Model input/X_daily_winter_384.npy')
Y=np.load(r'/content/gdrive/MyDrive/Colab Notebooks/CNN_Efficient_3_17_2023/Model input/Y_daily_winter_384.npy')

Xtrain = np.zeros((X.shape[0],256,256,2))
Ytrain = np.zeros((Y.shape[0],256,256,1))
for i in range(X.shape[0]):
   Xtrain[i]=X[i,:256,:256,:]
   Ytrain[i,...,0]=Y[i,:256,:256]

X,Y=[],[]
print(X.shape,Y.shape)

x_train,x_val,y_train,y_val=train_test_split(Xtrain,Ytrain,test_size=0.25)
x_train.shape

# select_id=[]
# for i in range(Y.shape[0]):
#     if np.count_nonzero(np.count_nonzero(Y[i]>0.1) >= 0.05*256*256):
#         select_id.append(i)
# len(select_id)

"""##DEM"""

from osgeo import gdal, ogr
dataset = gdal.Open(r'/content/gdrive/MyDrive/Colab Notebooks/DEM/DEM_input.tif')
band1 = dataset.GetRasterBand(1)
dem = band1.ReadAsArray()
dem.shape
dem[np.where(dem==32767)]=-99
#dem[np.where(dem<0)]=0
dem =(dem-0)/(np.max(dem)-0)*1
#dem = dem[:256,:256]

x_train= x_train1[...,0].reshape(x_train1.shape[0],256,256,1)
x_val= x_val1[...,0].reshape(x_val1.shape[0],256,256,1)
x_train.shape

x_train.shape

"""#Training

"""



import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping

# Define the early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=20)

# Create the training and validation datasets
train_ds = tf.data.Dataset.from_tensor_slices((x_train1, y_train))
val_ds = tf.data.Dataset.from_tensor_slices((x_val1, y_val))

# Define the batch size and preprocessing function
BATCH_SIZE = 8

def preprocess_data(image, label):
    image = tf.cast(image, tf.float32)
    label = tf.cast(label, tf.float32)
    return image, label

# Apply the preprocessing function, shuffle, batch, prefetch, and repeat the datasets
train_ds = train_ds.map(preprocess_data).shuffle(buffer_size=len(x_train1)).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)
val_ds = val_ds.map(preprocess_data).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)

#model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))
# Train the model using the iterators and the early stopping callback
history = model.fit(train_ds, validation_data=val_ds, epochs=100)

# Get training and test loss histories
training_loss = history.history['loss']
test_loss = history.history['val_loss']
# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# summarize history for loss
plt.figure(figsize=(10,8))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.ylim()
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

model.save('/content/gdrive/MyDrive/Colab Notebooks/Bias Correction Revision/Models/ef_nodem.h5')

from google.colab import runtime
runtime.unassign()

"""#Test"""

model = keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/CNN_Efficient_3_17_2023/Model_for_paper/Modelef_09_25_3hour_ver1.h5',compile=False)

model = keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/Bias Correction Revision/Models/ef_unet.h5')

model.summary()

path = "/content/gdrive/MyDrive/Colab Notebooks/PDIR_3hour/"
dir_list = os.listdir(path)
dir_list
print(path + dir_list[2])
print(path + dir_list[3])

data1 = Dataset(path + dir_list[2])
precip =data1.variables['precip'][:]
xtest1=np.zeros([248,737,1444])
for i in range(248):
    xtest1[i]=precip[i][2:739,0:1444]
lats = data1.variables['lat'][:]
lons = data1.variables['lon'][:]

data2 = Dataset(path + dir_list[3])
precip =data2.variables['precip'][:]
xtest2=np.zeros([224,737,1444])
for i in range(224):
    xtest2[i]=precip[i][2:739,0:1444]

fn2= '/content/gdrive/MyDrive/Colab Notebooks/ST4_hourly/st4_hourly_2021_jan.mat'
mat = io.loadmat(fn2)['st4_hourly']
ytest1=np.zeros([744,384,384])
for i in range(744):
    ytest1[i]=mat[:,:,i]

y_3hour=np.zeros([int(ytest1.shape[0]/3),384,384])
for i in range(int(ytest1.shape[0]/3)):
  y_3hour[i]= sum(ytest1[i*3:i*3+3,:,:],0)

xtest=xtest1
ytest=y_3hour
Xtest= np.zeros([xtest.shape[0],256,256,2])
Ytest= np.zeros([ytest.shape[0],256,256])
for i in range(248):
    a=xtest[i][9:393,0:384]
    b=ytest[i]
    idx=np.where(a==-99)
    a[idx]=-0.001
    b[np.where(np.isnan(b))]= 0
    b[idx]=-0.001
    a1=np.stack([a, dem],-1)
    Ytest[i]=b[:256,:256]
    Xtest[i]=a1[:256,:256,:]

ytest,xtest,data,mat=[],[],[],[]

test=model.predict(Xtest[...,0],batch_size=4)
test=test.reshape(test.shape[0],256,256).astype('float64')

nan_indx1 = np.where(Ytest[100]==-0.001)
count_indx1= len(nan_indx1[0])
count_indx1
#Adjust prediction
test[np.where(test<0)]=0
for i in range(Xtest.shape[0]):
  test[i][nan_indx1]=-0.001

# select_id=[]
# for i in range(Ytest.shape[0]):
#     if np.count_nonzero(Ytest[i]>0.1) >= 0.3*256*256:
#         select_id.append(i)
# select_id

"""##Visualize"""

lats=lats[11:267]
lons=lons[0:256]
mp= Basemap(projection = 'merc',
             llcrnrlon =lons[0],
             llcrnrlat = lats[255],
             urcrnrlon = lons[255],
             urcrnrlat = lats[0],
             resolution = 'i')
lon, lat = np.meshgrid(lons, lats)
x,y = mp(lon, lat)
#print(lats[0],lats[383],lons[0],lons[383])

for i in range(60):
  display_list = [Xtest[i][:,:,0], test[i], Ytest[i]]
  title = ['Input Image', 'Predicted Image','Ground Truth']

  plt.figure(figsize=(25,6))
  for j in range(3):
    plt.subplot(1, 3, j+1)
    plt.title(title[j],fontsize=15)
    plt.imshow(display_list[j],cmap='jet')
    plt.clim(0,4)
    if j==2:
      plt.colorbar()


  plt.show()

test.shape

for i in range(10):
  display_list = [Xtest[i][:,:,0], test[i], Ytest[i]]
  title = ['Input Image', 'Predicted Image','Ground Truth']

  plt.figure(figsize=(30,10))
  for j in range(3):
    plt.subplot(1, 3, j+1)
    plt.title(title[j],fontsize=25)
    mp.pcolor(x, y, display_list[j], cmap = 'jet')
    mp.drawparallels(np.arange(lats[255],lats[0],2), linewidth=0.1)#,fontsize=14,labels=[1,0,0,0])
    mp.drawmeridians(np.arange(lons[0],lons[255],3), linewidth=0.1)#,fontsize=14,labels=[0,0,0,1])
    mp.drawcoastlines()
    mp.drawstates()
    mp.drawcountries()
    #mp.drawlsmask(ocean_color='lightgrey', land_color=(0, 0, 0, 0), lakes=True, zorder = 2)
    plt.clim(0,3)

    if j==2:
      cbar = plt.colorbar()
      cbar.ax.tick_params(labelsize=16)
      cbar.set_label('mm/3-hour', labelpad=-40, y=1.05, rotation=0,fontsize=18)
    plt.axis('off')

  plt.show()

"""#Evaluation


"""

##
cm1=np.zeros([Ytest.shape[0],1])
cm2=np.zeros([Ytest.shape[0],1])
cm3=np.zeros([Ytest.shape[0],1])
cm4=np.zeros([Ytest.shape[0],1])
cm5=np.zeros([Ytest.shape[0],1])
cm6=np.zeros([Ytest.shape[0],1])

for i in range(Ytest.shape[0]):
  cm1[i] = np.corrcoef(Xtest[i,:,:,0].flatten(),Ytest[i].flatten())[0,1]
  cm2[i] = np.sqrt(mean_squared_error(Xtest[i,:,:,0].flatten(),Ytest[i].flatten()))
  cm3[i] = np.mean(np.abs(Xtest[i,:,:,0].flatten()-Ytest[i].flatten()))
  cm4[i] = np.corrcoef(test[i].flatten(),Ytest[i].flatten())[0,1]
  cm5[i] = np.sqrt(mean_squared_error(test[i].flatten(),Ytest[i].flatten()))
  cm6[i] = np.mean(np.abs(test[i].flatten()-Ytest[i].flatten()))

cm1 = np.ma.masked_where(np.isnan(cm1), cm1)
cm4 = np.ma.masked_where(np.isnan(cm4), cm4)

print(np.mean(cm1))
print(np.mean(cm2))
print(np.mean(cm3))
print(np.mean(cm4))
print(np.mean(cm5))
print(np.mean(cm6))

#CC, RMSE, MAE

test.shape

cor_fig=np.zeros([256,256])
rmse_fig=np.zeros([256,256])
mae_fig=np.zeros([256,256])
for i in range(256):
  for j in range(256):
    if test[1,i,j]==-0.001 :
      cor_fig[i,j]=np.nan;
      rmse_fig[i,j]=np.nan;
      mae_fig[i,j]=np.nan;
    else:
      cor_fig[i,j]=np.corrcoef(test[:,i,j],Ytest[:,i,j])[1,0];
      rmse_fig[i,j] = np.sqrt(mean_squared_error(test[:,i,j],Ytest[:,i,j]));
      mae_fig[i,j] = np.mean(np.abs(test[:,i,j]-Ytest[:,i,j]))
  print(i)

cor_fig[nan_indx1]=np.nan
rmse_fig[nan_indx1]=np.nan
mae_fig[nan_indx1]=np.nan
cor_fig = np.ma.masked_where(np.isnan(cor_fig), cor_fig)
rmse_fig = np.ma.masked_where(np.isnan(rmse_fig), rmse_fig)
mae_fig = np.ma.masked_where(np.isnan(mae_fig), mae_fig)

display_list = [cor_fig, rmse_fig , mae_fig]
title = ['CC', 'RMSE','MAE']

plt.figure(figsize=(14,5))
for i in range(3):
  plt.subplot(1, 3, i+1)
  plt.title(title[i],fontsize=20)
  mp.pcolor(x, y, display_list[i], cmap = 'jet')
  mp.drawparallels(np.arange(lats[255],lats[0],2), linewidth=0.2)#,fontsize=14,labels=[1,0,0,0])
  mp.drawmeridians(np.arange(lons[0],lons[255],3), linewidth=0.2)#,fontsize=14,labels=[0,0,0,1])
  mp.drawcoastlines()
  mp.drawstates()
  mp.drawcountries()
  plt.clim()

  cbar=plt.colorbar()
  cbar.ax.tick_params(labelsize=14)
  if i!= 0:
    cbar.set_label('mm/3-hour', labelpad=-40, y=1.06, rotation=0,fontsize=14)
  plt.axis('off')
plt.show()

print(np.mean(cor_fig))
print(np.mean(rmse_fig))
print(np.mean(mae_fig))

##POD, FAR, CSI

###Spatial Analysis
for i in range(Ytest.shape[0]):
  test[i][nan_indx1]=-0.001
  Xtest[i][nan_indx1]=-0.001
  Ytest[i][nan_indx1]=-0.001
from sklearn.metrics import confusion_matrix

a=test[:31,...].copy()
b=Ytest[:31,...].copy()
c=Xtest[...,0].copy()
threshold = 0.1

a[np.where(a<threshold)]= 0
a[np.where(a>=threshold)]=1
b[np.where(b<threshold)]=0
b[np.where(b>=threshold)]=1
c[np.where(c<threshold)]=0
c[np.where(c>=threshold)]=1

def matrix(a,b):
  m=confusion_matrix(b,a,labels=[True, False])
  return m

pod_fig=np.zeros([256,256])
far_fig=np.zeros([256,256])
csi_fig=np.zeros([256,256])


for i in range(256):
  for j in range(256):
    m=matrix(a[:,i,j],b[:,i,j]);
    if test[1,i,j]==-0.001 or np.sum(m[0,:])==0 or np.sum(m[:,0])==0:
      pod_fig[i,j]=np.nan;
      far_fig[i,j]=np.nan;
      csi_fig[i,j]=np.nan;
    else:
      pod_fig[i,j]=m[0,0]/np.sum(m[0,:]);
      far_fig[i,j]=m[1,0]/np.sum(m[:,0]);
      csi_fig[i,j]=m[0,0]/(np.sum(m[0,:])+m[1,0]);
  print(i)

pod_fig[nan_indx1]=np.nan
far_fig[nan_indx1]=np.nan
csi_fig[nan_indx1]=np.nan
pod_fig = np.ma.masked_where(np.isnan(pod_fig), pod_fig)
far_fig = np.ma.masked_where(np.isnan(far_fig), far_fig)
csi_fig = np.ma.masked_where(np.isnan(csi_fig), csi_fig)

display_list = [pod_fig, far_fig, csi_fig]
title = ['POD', 'FAR','CSI']
plt.figure(figsize=(14,5))
for i in range(3):
  plt.subplot(1, 3, i+1)
  plt.title(title[i],fontsize=20)
  mp.pcolor(x, y, display_list[i], cmap = 'jet')
  mp.drawparallels(np.arange(lats[255],lats[0],2), linewidth=0.2)#,fontsize=14,labels=[1,0,0,0])
  mp.drawmeridians(np.arange(lons[0],lons[255],3), linewidth=0.2)#,fontsize=14,labels=[0,0,0,1])
  mp.drawcoastlines()
  mp.drawstates()
  mp.drawcountries()
  plt.clim()
  plt.colorbar()
  cbar.ax.tick_params(labelsize=14)
  plt.axis('off')
plt.show()

print(np.mean(pod_fig))
print(np.mean(far_fig))
print(np.mean(csi_fig))

"""#Monthly Analysis

"""

##
cm1=np.zeros([Ytest.shape[0],1])
cm2=np.zeros([Ytest.shape[0],1])
cm=np.zeros([Ytest.shape[0],1])

for i in range(Ytest.shape[0]):
  cm1[i] = np.sum(Xtest[i,:,:,0].flatten())- np.sum(Ytest[i].flatten())#/np.sum(Ytest[i].flatten())
  cm2[i] = np.sum(test[i].flatten())- np.sum(Ytest[i].flatten())#/np.sum(Ytest[i].flatten())
  cm[i] = np.sum(Ytest[i].flatten())

print(np.mean(cm1))
print(np.mean(cm2))



for i in range(Ytest.shape[0]):
  test[i][nan_indx1]=np.nan
  Xtest[i][nan_indx1]=np.nan
  Ytest[i][nan_indx1]=np.nan
test = np.ma.masked_where(np.isnan(test), test)
Xtest = np.ma.masked_where(np.isnan(Xtest), Xtest)
Ytest = np.ma.masked_where(np.isnan(Ytest), Ytest)

##Accumulate 3hour to daily
test_day=np.zeros([int(test.shape[0]/8),256,256])
Xtest_day=np.zeros([int(test.shape[0]/8),256,256])
Ytest_day=np.zeros([int(test.shape[0]/8),256,256])
for i in range(int(test.shape[0]/8)):
  test_day[i]= sum(test[i*8:i*8+8,:,:],0)
  Xtest_day[i]= sum(Xtest[i*8:i*8+8,:,:,0],0)
  Ytest_day[i]= sum(Ytest[i*8:i*8+8,:,:],0)

for i in range(Ytest_day.shape[0]):
  test_day[i][nan_indx1]=np.nan
  Xtest_day[i][nan_indx1]=np.nan
  Ytest_day[i][nan_indx1]=np.nan
test_day = np.ma.masked_where(np.isnan(test_day), test_day)
Xtest_day = np.ma.masked_where(np.isnan(Xtest_day), Xtest_day)
Ytest_day = np.ma.masked_where(np.isnan(Ytest_day), Ytest_day)

display_list = [np.sum(Xtest_day,0), np.sum(test_day,0), np.sum(Ytest_day,0)]
#display_list = [Xtest_day[5], test_day[5], Ytest_day[5]]
title = ['Input Image', 'Predicted Image','Ground Truth']

plt.figure(figsize=(30,10))
for i in range(3):
  plt.subplot(1, 3, i+1)
  plt.title(title[i],fontsize=25)
  mp.pcolor(x, y, display_list[i], cmap = 'jet')
  mp.drawparallels(np.arange(lats[255],lats[0],2), linewidth=0.4)#,fontsize=14,labels=[1,0,0,0])
  mp.drawmeridians(np.arange(lons[0],lons[255],3), linewidth=0.4)#,fontsize=14,labels=[0,0,0,1])
  mp.drawcoastlines()
  mp.drawstates()
  mp.drawcountries()
  plt.clim(0,250)
  if i==2:
      cbar = plt.colorbar()
  plt.axis('off')
  cbar.ax.tick_params(labelsize=16)
  cbar.set_label('mm/month', labelpad=-40, y=1.05, rotation=0,fontsize=18)
  #cbar.ax.set_title('mm/month',fontsize=18)
plt.show()

print(np.mean(np.sum(Xtest_day,0)))
print(np.mean(np.sum(test_day,0)))
print(np.mean(np.sum(Ytest_day,0)))

"""#Daily model"""

model = keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/CNN_Efficient_3_17_2023/Model_for_paper/Modelef_09_25_daily_ver1.h5',compile=False)

data3 = Dataset(r'/content/gdrive/MyDrive/Colab Notebooks/PDIR_Now/PDIR_United_States_2022-12-22121909pm_2021.nc')
precip =data3.variables['precip'][:]
time = data3.variables['datetime'][:]
xtest2=np.zeros([365,737,1444])
for i in range(297):
    xtest2[i]=precip[i][2:739,0:1444]
for i in range(298,365):
    xtest2[i]=precip[i-1][2:739,0:1444]
#Missing data at index 297 for 2021
xtest2[297]= (xtest2[296]+xtest2[298])/2
lats = data3.variables['lat'][:]
lons = data3.variables['lon'][:]

fn2= '/content/gdrive/MyDrive/Colab Notebooks/ST4/ST4_2021_west.mat'
mat = io.loadmat(fn2)['st4_daily']
ytest2=np.zeros([365,384,384])
for i in range(365):
    ytest2[i]=mat[:,:,i]

xtest=xtest2
ytest=ytest2
Xtest= np.zeros([xtest.shape[0],256,256,2])
Ytest= np.zeros([ytest.shape[0],256,256])
for i in range(xtest.shape[0]):
    a=xtest[i][9:393,0:384]
    b=ytest[i]
    idx=np.where(a==-99)
    a[idx]=-0.001
    b[np.where(np.isnan(b))]= 0
    b[idx]=-0.001
    a1=np.stack([a, dem],-1)
    Ytest[i]=b[:256,:256]
    Xtest[i]=a1[:256,:256,:]

test=model.predict(Xtest,batch_size=4)
test=test.reshape(test.shape[0],256,256).astype('float64')
nan_indx1 = np.where(Ytest[100]==-0.001)
count_indx1= len(nan_indx1[0])
count_indx1
#Adjust prediction
test[np.where(test<0)]=0
for i in range(Xtest.shape[0]):
  test[i][nan_indx1]=-0.001

for i in range(100):
  display_list = [Xtest[i][:,:,0], test[i], Ytest[i]]
  title = ['Input Image', 'Predicted Image','Ground Truth']

  plt.figure(figsize=(25,6))
  for j in range(3):
    plt.subplot(1, 3, j+1)
    plt.title(title[j],fontsize=15)
    plt.imshow(display_list[j],cmap='jet')
    plt.clim(0,20)
    if j==2:
      plt.colorbar()


  plt.show()

"""#Log

Test 03/21/2023
"""

#Test1: Input 3hour data 256x256 with DEM, no select 10% , CNN efficietnet model, activate function relu
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019
#Model
0.4015038652847302
0.960507039944446
0.3878910354699161

0.5748125542679322
0.6223559965561438
0.28575440294078747

##Test2: Get weight from model trained with 384x384 CNN efficientnet model, activate function mish, and scale down to 256x256
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019
#Model
0.38199498438127677
1.0602249645255077
0.49179941871953164

0.8267140745615029
0.7281353860589745
0.25578254876154266

"""Test 03/22/2023"""

#Test3: Input 3hour data 256x256 with DEM, no select 10% , CNN efficietnet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019

#Model: Best
0.4095790089705213
0.9579301147257314
0.40053506101805353

0.6740259486047396
0.6637448652023569
0.2840791199932215

#Test4 Input 3hour data 256x256 with DEM, select 10% , CNN efficietnet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019

#Model:
0.40909347827408615
0.9800935812085303
0.4670237675162576

0.8816389738908118
0.7864431460770753
0.2070393323766918

#Test5 Input 3hour data 256x256 with DEM, select 10% , CNN unet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019

#Model:
0.3084733970516633
1.0434783194783361
0.5258178808700845

0.8241424161840788
0.7826435922138353
0.20270813312585495

"""
Test 04/03/2023"""

#Test6 Input 3hour data 256x256 without DEM, no select 10% , CNN-efficient unet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019

#Model:
0.4068105135400206
0.9972741273748296
0.38307626457795935

0.6202918430864229
0.6375354119434405
0.29984720486283534


#Test7 Input 3hour data 256x256 with DEM, no select 10% , CNN-efficient unet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019

#Model:
0.4009529761215517
0.9673733866713912
0.41364223447618753

0.683277958589333
0.6835007821798368
0.27371367642712296

"""Test 04/05/2023"""

#Test8 Input daily data 256x256 with DEM, no select 10% , CNN-efficient unet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.4439403966196868
6.048393409160428
3.5743700836068677

0.9472193523897583
0.5795348703214214
0.4130764210448705
#Model:
0.6630617755402156
4.001612264601636
2.052574006834884

0.6214064753418334
0.2721451647912362
0.4918603189816812

#Test8 Input daily data 256x256 with DEM, no select 10% , CNN-efficient unet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
0.4439403966196868
6.048393409160428
3.5743700836068677

0.9472193523897583
0.5795348703214214
0.4130764210448705
#Model:
0.656656038173428
4.045857498065171
2.1200375771129316

0.8207839954031438
0.4600206111409078
0.4726430728959523



#Test9 Input hourly data 256x256 with DEM, no select 10% , CNN-efficient unet model, activate function mish
#CC, RMSE, MAE , POD, FAR, CSI
#PDIR:
#PDIR:
0.2614991898224234
1.6045808307486602
0.7438147709180309

0.6971008372961502
0.726003970313722
0.2434268723340019
#Model:
0.39233737225075666
0.9714050060164918
0.40410307870203727

0.6637926908126648
0.686657209175074
0.2593929025051856